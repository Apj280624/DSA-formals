#include <bits/stdc++.h>
using namespace std;
#define fastio()                      \
    ios_base::sync_with_stdio(false); \
    cin.tie(NULL);                    \
    cout.tie(NULL);
#define lli long long int
#define ll long long
#define pii pair<int,int>
#define pll pair<ll,ll>
#define tpiii tuple<int,int,int>
#define tplll tuple<ll,ll,ll>
// DS
#define vi vector<int>
#define vb vector<bool>
#define vl vector<ll>
#define vvi vector<vi>
#define vvl vector<vl> 
#define umap unordered_map
#define uset unordered_set
#define mset multiset
// Utilities
#define ff first
#define ss second
#define bg begin()
#define ed end()
#define pb push_back
#define ppb pop_back
ll mod1=1e9+7;
ll mod2=1e9+7;
// Functions
ll mod_add(ll x,ll y,ll m) { return ((x%m)+(y%m))%m; }
ll mod_mul(ll x,ll y,ll m) { return ((x%m)*(y%m))%m; }

void my_swap(int& x,int& y) {
    if(&x == &y) {
        return; // this is imp if both x and y are references to the same var
    }
    
    x = x^y;
    y = x^y;
    x = x^y;
}


/////////////////// bubble, selection and insertion sort //////////////////

/*
all three by apna college: https://www.youtube.com/watch?v=PkJIc5tBRUE
microsoft vali didi

BUBBLE SORT:
name comes from the fact the bubbles in water reach the top, similarily here we push the heavier elements to the right

algo:
for n-1 phases:
traverse from left to right, starting from the first one, repeatedly swap consecutive elements (left,right)
if left element is greater than the right one
so, the heaviest element in the current array moves to the last of the current array
as heavier elements reach to the last array size decreases by one after each phase

optimization:
if at any phase there was no swapping done, then we can stop here itself and we don't need to execute other phases
we can use a flag here

why it works:
in the first phase the heaviest element gets pushed to last index and array on which we need to operate decrease by one 
in the second phase the second heaviest element get pushed to the second last index and array on which we need to operate 
decrease by one. we need only n-1 phases, because if we put n-1 elements at their correct places remaining element
already comes at its correct place

time complexity and stability: 
worst, avg case time = o(n^2)
best case time = o(n)
space = o(1)
best case => array is already sorted, if we optimize then best case time = o(n) when array is fully sorted, 
optimization is done by using swapped flag, if we remove that optimization is removed
worst case => array is sorted in descending order
stable = yes => 
when we talk about it stability, its about relative position of equal elements 
consider a case with two elements with value at 8 at indices 3 and 6, the 8 present at index 3 wont ever cross 8 
present at index 6 because the if condition doesnot have any equality sign

////////////////////////////////////////////////////////////////////////

INSERTION SORT:

algo:
array is divided into two regions sorted and unsorted, initially whole array is unsorted, unsorted region starts from i, 
we traverse from left to right, we need to 'insert' the first element of current unsorted region (ie at the element at index i) 
at its correct index by right-shifting the elements of sorted array one by one if they are larger 
(not by swapping even though it could give the same result)

optimization:
no optimization needed, inner loop will only work if we have larger elements in left of index i

time complexity and stability: (best case and worst case scenarios and time complexity same as bubble sort)
worst, avg case time = o(n^2)
best case time = o(n)
space = o(1)
best case => array is already sorted, in best case inner loop doesnot work at all
worst case => array is sorted in descending order
stable = yes => 
when we talk about it stability, its about relative position of equal elements 
consider a case with two elements with value 2 at indices 4 and 5, and elements to the left of index 4 are greater than 2, 
then the 2 present at index 4 will reach to the first position and the one at 5 will react at the
second position from left and the 2 which was originally present at index 5 will never cross the one originally present
at index 4

extra:
in all cases other than the best case, both the loop runs till their bounds even though swapping occurs or not
only the best case the outer loop runs once so time = o(n)

bubble sort and insertion sort use consecutive swapping or shifting that's why they are stable

//////////////////////////////////////////////////////////////////
 
SELECTION SORT:

algo:
array is divided into two regions sorted and unsorted, initially whole array is unsorted, unsorted region starts from i,
each time 'select' the index of minimum element from the the unsorted region and put it at last of sorted region by swapping

this works some what exactly opposite to bubble sort in results of each but implementation is different
in the first iteration, first minimum element comes to the 0th index
in the second iteration, second minimum element comes to the 1st index and so on

time complexity and stability: 
best, worst and avg case time = o(n^2), which is independent of the distribution of data
best case time = o(n)
space = o(1)

stable: no => because here we are swapping distant elements unlike bubble and insertion sort
example: 1 3 3 2 2, and i = 1, we find the index of the minimum element = 2 (element = 2)
we swap it with 3 at index 1, so the 3 at index 1 crosses the 3 at index 2


///////////////////////////////////////////////////////////////////////

COUNT SORT:

algo:
store count of all elements from left to right, convert count to prefix count
now count actually has the position of the elements, how:
we have count[x] this means element 'x' will lie in the array from position
count[x-1]+1 to count[x]

example: 1 1 1 3 3 3 
count=[0,3,3,6]
3 lies in positions from count[2]+1 to count[3] ie from 4 to 6

now we will start to fill output array(we need to create and return seperate array) lets say we are at index 'i'
we know the current count of v[i]=count[v[i]]=x, then index of v[i] is x-1
so we can keep v[i] at index x-1 in the output array
and we can reduce the count[v[i]] as we have used one

time complexity: o(n+range of input), independent of distribution of data

stability: depends on the implementation
if we fill output from left to right, then it looses its stability
if we fill from right to left, then its stable

why: lets do more work to visualize
lets say in the first step when we were storing count of each element
we also create a map<int,vector<pii>> element
so at each 'i' we do: count[v[i]]++; and element[v[i]].push_back({v[i],i});
here we are using push_back which pushes at back

convert count to prefix same as before and 

now if we fill the output array from left to right and do:
        int new_idx = count[v[i]]-1;
        ans[new_idx] = element[v[i]].back().first
        element[v[i]].pop_back();
        count[v[i]]--;
        
you can see the element which was pushed last in the map got added to the array to a higher index than the element which was pushed first during making map

in the order words we can say that if we fill from left to right, and consider
the same element (ie elements with same value), lets says its has two occurences
one at index 1 and other at index 5, we make count from left to right
so count for index 1 will be increment before 5, but when we fill output 
from left to right you decrement the count which was contributed by the
occurence of that element at index 5 before the occurrence at index 1


//////////////////////////////////////////////////////////////////////

MERGE SORT:

time complexity and stability: 
best, worst and avg case time = o(n^2), which is independent of the distribution of data
stable: yes => it could be made stable, rearrangement of elements takes place while merging so we need to make chngs there, 
while we put the elements into the big array, we select the minimum b/n the two there we have

so if(a[i]<=b[j]) {
    i think that :here we must use equality sign also, so that even if two  elements are equal
    a[i] goes first because a[i] was left to b[j] because 'a' array is left to b
    that's it
} else {
    
}
 
Several common sorting algorithms are stable by nature, such as Merge Sort, Timsort, Counting Sort, Insertion Sort, 
and Bubble Sort. Others such as Quicksort, Heapsort and Selection Sort are unstable.


bubble, insertion, count, merge sort -> stable
selection, quick sort -> unstable

comparision based: bubble, insertion, selection, merge sort
non comparision based: radix, count sort

*/

//////////////////////////// bubble sort /////////////////////////////////


void bubble_sort(vector<int>& v) {
    int n = v.size();
    
    for(int i = 0; i < n-1; i++) {
        bool swapped = false;
        
        for(int j = 1; j < n-i; j++) {
            if(v[j-1] > v[j]) {
                // v[i] needs to go back
                my_swap(v[j-1], v[j]);
                swapped = true;
            }
        }
        
        if(!swapped) {
            // array has been sorted
            break;
        }
    }
}

//////////////////////////// selection sort /////////////////////////////////

void selection_sort(vector<int>& v) {
    int n = v.size();
    
    // i marks the start of unsorted region
    for(int i=0; i < n-1 ; i++) {
        int sm_idx = i; // idx of smallest element in the unsorted region
        
        for(int j = i+1; j < n ; j++) {
            if(v[j] < v[sm_idx]) {
                sm_idx = j;
            }
        }
        
        my_swap(v[i], v[sm_idx]);
    }
}

//////////////////////////// insertion sort /////////////////////////////////

void insertion_sort(vector<int>& v) {
    int n = v.size();
    
    // i marks the start of unsorted region
    for(int i = 1; i < n; i++) {
        int who = v[i];
        int j = i-1;
    
        while(j >= 0 && v[j] > who) {
            v[j+1] = v[j];
            j--;
        }
        
        j++;
        v[j] = who;
    }
}

///////////////////////////// count sort ///////////////////////////////

vector<int> count_sort(vector<int> v) {
    int n = v.size();
    int maxi = 0;
    for(auto x:v) {
        maxi = max(maxi,x);
    }
    
    int m = maxi+1;
    vector<int> count(m);
    for(auto x:v) {
        count[x]++;
    }
    
    for(int i=1; i<m; i++) {
        count[i] += count[i-1];
    }
    
    vector<int> ans(n);
    // for stability right to left
    for(int i=n-1; i>=0; i--) {
        int new_idx = count[v[i]]-1;
        ans[new_idx] = v[i];
        count[v[i]]--;
    }
    
    return ans;
}


//////////////////////////// quick sort /////////////////////////////////

/*

apna college youtube vdo:
https://www.youtube.com/watch?v=Dl6HT-NM_q4

placement course

divide and conquer recursive algo

algo:
choose a pivot(same for always) here we choose last element always 
partition the array around the pivot, ie all smaller elements on the left of pivot and all larger elements to the right of pivot
now recursively sort the left and right regions of pivot

algorithm for partition is important to look

i haven't used the partitioning algo from apna college
i have used the algo used for the standard problem from the pepcoding youtube video:
https://www.youtube.com/watch?v=if40LxQ8_Xo 

algo: 

i to n-1 => unknown region or the region on which we are operating 
j to i-1 => elements greater than pivot
0 to j-1 => elements lesser than pivot

here i is being incremented inside the for loop
code by sumit sir increments i inside the while loop body

best and avg case time comp = o(nlogn)

worst case time comp = o(n^2), this occurs when the given array is already asc or desc sorted, and the pivot is always the smallest or largest element

consider a case when given array is already sorted and pivot is the last element ie the largest one, in such case when we partition all the n-1 elements other than the last element will go in one partition and we will apply quick sort on an array of n-1 size, so we were just able to remove one element after partioning

best and avg cases include when partitioning happens such that the array gets divided into two equal sized arrays like in merge sort

Though the worst-case complexity of quicksort is more than other sorting algorithms such as Merge sort and Heap sort, still it is faster in practice. Worst case in quick sort rarely occurs because by changing the choice of pivot, it can be implemented in different ways. Worst case in quicksort can be avoided by choosing the right pivot element.

https://www.javatpoint.com/quick-sort#:~:text=Time%20Complexity,-Case&text=The%20average%20case%20time%20complexity,O(n*logn).

Space Complexity	O(n*logn)
stable: no => because here we are swapping distant elements unlike bubble and insertion sort

*/

int partition(vector<int>& v, int l, int r) {
    int n = v.size(), pvt = v[r];
    
    int j = l;
    for(int i=l; i<=r; i++) {
        if(v[i] <= pvt) {
            my_swap(v[i], v[j]);
            j++;
        }
    }
    
    return j-1; // assuming there's a pvt element for sure
}

void quick_sort(vector<int>& v, int l, int r) {
    if(l >= r) {
        return;
    }
    
    int pvt_idx = partition(v, l, r);
    
    quick_sort(v, l, pvt_idx-1);
    quick_sort(v, pvt_idx+1, r);
}

void print(vector<int>& v) {
    for(auto x: v) {
        cout << x << " ";
    }
    
    cout<<'\n';
}

vector<int> getArray() {
    vector<int> v = {1,2,3,4,5};
    
    v = {7,8,8,4,3,1,2,4,11,1};
    
    return v;
}

void solve()
{
    vector<int> v = getArray();
    int n=v.size();
    
    v = getArray(); v = count_sort(v); print(v);
    
    v = getArray(); bubble_sort(v); print(v);
    
    v = getArray(); selection_sort(v); print(v);
    
    v = getArray(); insertion_sort(v); print(v);
    
    v = getArray(); quick_sort(v, 0, n-1); print(v);
    
    return;
}

int main()
{
    fastio();
    int t=1;
    // cin>>t;
    while(t--) solve();

    return 0;
}
